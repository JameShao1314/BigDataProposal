\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}
%\usepackage{ifpdf}


\usepackage{cite}
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  \usepackage{epstopdf}
\else
  \usepackage[dvips]{graphicx}
\fi

%\usepackage{amsmath}

%\usepackage{algorithmic}

%\usepackage{array}

%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi

%\usepackage{fixltx2e}
%\usepackage{stfloats}
%\fnbelowfloat
%\usepackage{dblfloatfix}

%\usepackage{url}



\begin{document}
\title{Image Captioning based on Recurrent Neural Network Model}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Heng Qiao}
\IEEEauthorblockA{Department of Electrical\\ and Computer Engineering\\
University of Florida\\
Email:}
\and
\IEEEauthorblockN{Tong Shao}
\IEEEauthorblockA{\\
Email: }
\and
\IEEEauthorblockN{Yichen Liang}
\IEEEauthorblockA{\\
Email: }}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Image captioning is the task to automatically describe the content of an image, which is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this course project, we plan to develop an image captioning system based on the recurrent neural network(RNN) Model. As the most widely used scheme, two neural networks are introduced. The first one is a pre-trained convolutional neural network (CNN) that converts the image into feature vectors, such as the VggNet, ResNet and Inception. Serving as the core of the scheme, the second one adopts the recurrent neural network (RNN) model. It takes the image feature as the input and generate the word vectors of a sentence (caption). The model is trained to maximize the likelihood of the target description sentence given the training image. Based on this, we aim at implementing some assistant techniques to further improve the performance, such as the semantic structures and more optimized neural network structures. This scheme will be implemented in Tensorflow. Experiments will be conducted on public dataset MS COCO and related evaluation scores such as the BLEU-4 score will be provided. Also, the final report, slides and other related material will be prepared as well.
\end{abstract}

\IEEEpeerreviewmaketitle
goals, deliverables, overview architecture, use cases, modules/subsystems, flowcharts, algorithms, potential results (performance metrics, success evaluation criteria, mock-up result figures x,y,z curves etc.).


\section{Introduction}

Deep learning technology has been widely used in many aspects \cite{krizhevsky2012imagenet}, such as face recognition, natural language processing and etc. It has shown remarkable learning ability in many areas. Among them, the most common form of machine learning, deep or not, is supervised learning \cite{lecun2015deep}. Supervised learning requires labeled training data and typically outputs a label when given a new input \cite{lecun1998gradient}. To deal with inputs with correlations in time such as an sentence, recurrent neural network (RNN) \cite{mikolov2010recurrent}, typically implemented with long short-term memory (LSTM) units, is employed.

Image captioning is the procedure to automatically describe the content of an image, as shown in Fig. \ref{cap_fig}. And there has been a recent surge of interest in developing models that can generate captions for images or videos. Most of these approaches learn a probabilistic model of the caption, conditioned on an image or a video. Most recent work on visual captioning first extracts features from an image, obtaining a fixed-length vector representation of a given image or video. Then a language, usually a recurrent neural network (RNN), typically implemented with long short-term memory (LSTM) units \cite{graves2013speech}, is employed to generate a caption while the image's vector representation is the input \cite{vinyals2015show}.

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figures/caption.pdf}
  \caption{Examples of image captioning.}\label{cap_fig}
\end{figure}

Though good results have been achieved, most methods are still far away from human performance and the models usually don't consider the grammar rules. Recent work shows that adding explicit high-level semantic concepts of the input image/video can further improve visual captioning. For instance, in \cite{you2016image}, a model of semantic attention is proposed which selectively attends to semantic concepts through a soft attention mechanism.

We aim at designing a typical image captioning system while adding some basic grammar rules to further enhance the performance. 

\subsection{Subsection Heading Here}
Subsection text here.


\subsubsection{Subsubsection Heading Here}
Subsubsection text here.




\section*{Acknowledgment}


The authors would like to thank...
\bibliographystyle{IEEEtran}
\bibliography{refs}


% that's all folks
\end{document}


